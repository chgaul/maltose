{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ase.io\n",
    "import torch\n",
    "import schnetpack\n",
    "\n",
    "from maltose.atoms import MultitaskAtomsData\n",
    "\n",
    "import multitask_data\n",
    "import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "base_dir = '..'\n",
    "data_base_dir = os.path.join(base_dir, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dir(model_name):\n",
    "    return os.path.join(base_dir, 'models', model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "def load_model(model_name):\n",
    "    dir = model_dir(model_name)\n",
    "    files = os.listdir(dir)\n",
    "    if 'best_model_state.pth' in files:\n",
    "        module_name = 'configs.{}'.format(model_name)\n",
    "        print('Importing module {}...'.format(module_name))\n",
    "        config = importlib.import_module(module_name)\n",
    "        model = config.build_model()\n",
    "        path = os.path.join(dir, 'best_model_state.pth')\n",
    "        print('Loading', path)\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        model.eval()\n",
    "        return model\n",
    "    elif 'best_model' in files:\n",
    "        path = os.path.join(model_dir(model_name), 'best_model')\n",
    "        print('Loading', path)\n",
    "        model = torch.load(path, map_location=device)\n",
    "        model.eval()\n",
    "        return model\n",
    "    else:\n",
    "        print('model_name not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v08_sum'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v01'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on unified test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_est = evaluation.compute_regular_data(model, n_points=10, seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on data of Kuzmich2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_xyz(model, xyzfile):\n",
    "    return model.forward(\n",
    "        schnetpack.data.loader._collate_aseatoms([\n",
    "            schnetpack.data.atoms.torchify_dict(\n",
    "                schnetpack.data.atoms._convert_atoms(\n",
    "                    ase.io.read(xyzfile)\n",
    "                )\n",
    "            )\n",
    "        ])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kuzmich(model, seed=None, n_points=-1):\n",
    "    kuzmich_dir = os.path.join(data_base_dir, 'Kuzmich2017')\n",
    "    df = pd.read_csv(os.path.join(kuzmich_dir, 'table1.csv'))\n",
    "    mapping = {\n",
    "        'DTDfBTTDPP2': 'DTDfBT(TDPP)2',\n",
    "        '10_DBFI-MTT': 'DBFI-MTT',\n",
    "    }\n",
    "    ambiguous = ['M10']\n",
    "    \n",
    "    # Get valid files and establish canonical order\n",
    "    files = {}\n",
    "    for f in sorted(os.listdir(os.path.join(kuzmich_dir, 'xyz'))):\n",
    "        if f.endswith('.xyz'):\n",
    "            id = f[3:-13]\n",
    "            if id in mapping:\n",
    "                id = mapping[id]\n",
    "            if id in ambiguous:\n",
    "                print('id: {} ambiguous'.format(f[3:-13]))\n",
    "                continue\n",
    "            lb = f[3:-13]\n",
    "            files[lb] = (f, id)\n",
    "    # Sort by keys:\n",
    "    fs = sorted(list(files.items()))\n",
    "\n",
    "    # Shuffle order\n",
    "    if seed is not None:\n",
    "        random_state = np.random.RandomState(seed=seed)\n",
    "        random_state.shuffle(fs)\n",
    "\n",
    "    # Compute only on the desired random subset\n",
    "    ret = {}\n",
    "    for lb, (f, id) in fs[:n_points]:\n",
    "        xyzfile = os.path.join(kuzmich_dir, 'xyz', f)\n",
    "        pred = predict_on_xyz(model, xyzfile)\n",
    "        est = {k: float(v) for k, v in pred.items()}\n",
    "        tgt = {\n",
    "            'LUMO-B3LYP': float(df[df['Acceptorâ€™s Label']==id]['LUMO (eV)'])\n",
    "        }\n",
    "        ret[lb] = {\n",
    "            'tgt': tgt,\n",
    "            'est': est,\n",
    "        }\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring data into the regular format and add it to the\n",
    "# target-estimates collection:\n",
    "def add_kuzmich(tgt_est: dict, seed: int = None, n_points: int = -1):\n",
    "    tgt_est_kuzmich = evaluate_kuzmich(model, seed=seed, n_points=n_points)\n",
    "    # Drop keys:\n",
    "    k_data = list(tgt_est_kuzmich.values())\n",
    "    ret = {\n",
    "            'tgt': {p: np.array([]) for p in k_data[0]['tgt'].keys()},\n",
    "            'est': {p: np.array([]) for p in k_data[0]['est'].keys()},\n",
    "        }\n",
    "    for kd in k_data:\n",
    "        for k in ret['tgt'].keys():\n",
    "            ret['tgt'][k] = np.append(ret['tgt'][k], [kd['tgt'][k]])\n",
    "        for k in ret['est'].keys():\n",
    "            ret['est'][k] = np.append(ret['est'][k], [kd['est'][k]])\n",
    "    tgt_est['Kuzmich2017'] = ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlined evaluation and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOMSEED = 26463461"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v08'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v08_avg'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v08_sum'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v06'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_only_b3lyp'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v07'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v01'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v01_sum'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v01b_avg'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v05'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_only_pbe0'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_path = os.path.join(\n",
    "    model_dir(model_name), 'best_model_state.pth')\n",
    "if not os.path.exists(state_path):\n",
    "    print(\"Save also the model's also as state dictionary:\", state_path)\n",
    "    torch.save(model.state_dict(), state_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute and dump the full error distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_file = os.path.join(model_dir(model_name), 'deviations.npz')\n",
    "summary_file = os.path.join(model_dir(model_name), 'deviations_summary.json')\n",
    "if not os.path.exists(summary_file) and not os.path.exists(target_file):\n",
    "    est_properties = evaluation.get_available_properties(model=model)\n",
    "    tgt_est = evaluation.compute_regular_data(model, n_points=None, seed=RANDOMSEED)\n",
    "    add_kuzmich(tgt_est, seed=RANDOMSEED)\n",
    "    devs = {}\n",
    "    for test, data in tgt_est.items():\n",
    "        print(test)\n",
    "        for p in data['tgt'].keys():\n",
    "            if p in data['est']:\n",
    "                print('  ', p)\n",
    "                devs[test + ':' + p] = data['est'][p] - data['tgt'][p]\n",
    "    np.savez(target_file, **devs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the deviations (DataFrame, json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(summary_file):\n",
    "    devs = np.load(target_file)\n",
    "    import pandas as pd\n",
    "    summary = pd.DataFrame(columns=['test', 'property', 'mean(error)', 'std(error)', 'MAE', 'RMSE', 'size'])\n",
    "    for k, dev in devs.items():\n",
    "        test, p = k.split(':')\n",
    "        summary = pd.concat([\n",
    "            summary,\n",
    "            pd.DataFrame({\n",
    "                'test': test,\n",
    "                'property': p,\n",
    "                'mean(error)': np.mean(dev),\n",
    "                'std(error)': np.std(dev),\n",
    "                'MAE': np.mean(np.abs(dev)),\n",
    "                'RMSE': np.sqrt(np.mean(np.square(dev))),\n",
    "                'size': len(dev),\n",
    "            }, index=[0])], ignore_index=True)\n",
    "    summary.to_json(summary_file, indent=2, orient='records')\n",
    "else:\n",
    "    summary = pd.read_json(os.path.join(model_dir(model_name), 'deviations_summary.json'))\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 25\n",
    "est_properties = evaluation.get_available_properties(model=model)\n",
    "tgt_est = evaluation.compute_regular_data(model, n_points=n_points, seed=RANDOMSEED)\n",
    "add_kuzmich(tgt_est, seed=RANDOMSEED, n_points=n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a fixed color code for each test set\n",
    "for k, color in {\n",
    "    'qm9': 'orange',\n",
    "    'alchemy': 'red',\n",
    "    'oe62': 'purple',\n",
    "    'hopv': 'blue',\n",
    "    'TABS': 'green',\n",
    "    'Kuzmich2017': 'black'\n",
    "}.items():\n",
    "    if k in tgt_est:\n",
    "        tgt_est[k]['color'] = color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = ['HOMO', 'LUMO', 'Gap']\n",
    "theories = ['B3LYP', 'PBE0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(tgt_est: dict, qt_tgt: str, qt_est: str, n_points: int = -1, skiptests=[]):\n",
    "    def measure_errors(x, y):\n",
    "        dev = np.array(x) - np.array(y)\n",
    "        mae = '{:.2f}eV'.format(np.mean(np.abs(dev)))\n",
    "        rmse = '{:.2f}eV'.format(np.sqrt(np.mean(np.square(dev))))\n",
    "        return mae, rmse\n",
    "    def lookup_errors(test, prop):\n",
    "        test_row = summary[(summary['test']==test) & (summary['property']==prop)]\n",
    "        assert len(test_row) == 1\n",
    "        mae = '{:.3f}eV'.format(float(test_row['MAE']))\n",
    "        rmse = '{:.3f}eV'.format(float(test_row['RMSE']))\n",
    "        return mae, rmse\n",
    "\n",
    "    plot_empty = True\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plotname = '{}-{}'.format(model_name, qt_tgt);\n",
    "    if qt_est != qt_tgt:\n",
    "        plotname += '-cross'\n",
    "    for dataset_name, te in tgt_est.items():\n",
    "        try:\n",
    "            x = te['tgt'][qt_tgt]\n",
    "            y = te['est'][qt_est]\n",
    "        except:\n",
    "            continue\n",
    "        if dataset_name in skiptests:\n",
    "            # Add a tag, but only if skipped due to skiptests: \n",
    "            plotname = '{}-skip{}'.format(plotname, dataset_name)\n",
    "            continue\n",
    "        if qt_tgt == qt_est:\n",
    "            try:\n",
    "                mae, rmse = lookup_errors(test=dataset_name, prop=qt_est)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"\"\"Something went wrong looking up {}, {}. Measure \n",
    "                errors from plot data.\"\"\".format(dataset_name, qt_est))\n",
    "                mae, rmse = measure_errors(x, y)\n",
    "        else:\n",
    "            mae, rmse = measure_errors(x, y)\n",
    "        print('{}: MAE={}, RMSE={}'.format(dataset_name, mae, rmse))\n",
    "        plt.scatter(x[:n_points], y[:n_points], color=te['color'], label='{dataset} (MAE={mae})'.format(\n",
    "            dataset=dataset_name, mae=mae))\n",
    "        plt.axline((np.mean(x), np.mean(x)), slope=1)\n",
    "        plt.xlabel('{} target (eV)'.format(qt_tgt))\n",
    "        plt.ylabel('{} estimate (eV)'.format(qt_est))\n",
    "        plot_empty = False\n",
    "    if plot_empty:\n",
    "        print(\"{}/{} empty for {}.\".format(qt_tgt, qt_est, model_name))\n",
    "    else:\n",
    "        plt.title(model_name)\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        tgt_dir = os.path.join(base_dir, 'figures', 'tgt-est')\n",
    "        os.makedirs(tgt_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(tgt_dir, '{}.pdf'.format(plotname)), dpi=200)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target-estimate plots for each property and theory (diagonal and cross)\n",
    "for a in properties:\n",
    "    for t in theories:\n",
    "        assert len(theories)==2\n",
    "        t_cross = [th for th in theories if th != t][0]\n",
    "        q = a + '-' + t\n",
    "        q_cross = a + '-' + t_cross\n",
    "        plt.rcParams.update({'axes.facecolor': 'lightgray'})\n",
    "        make_plot(tgt_est, q, q_cross, n_points=n_points)\n",
    "        plt.rcParams.update({'axes.facecolor': 'white'})\n",
    "        make_plot(tgt_est, q, q, n_points=n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schnetpack-pip",
   "language": "python",
   "name": "schnetpack-pip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
