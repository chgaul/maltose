{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ase.io\n",
    "import torch\n",
    "import schnetpack\n",
    "\n",
    "from maltose.atoms import MultitaskAtomsData\n",
    "import maltose as schnetpack_custom\n",
    "\n",
    "import multitask_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "base_dir = '..'\n",
    "data_base_dir = os.path.join(base_dir, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b3lyp_tasks = ['HOMO-B3LYP', 'LUMO-B3LYP', 'Gap-B3LYP']\n",
    "pbe0_tasks = ['HOMO-PBE0', 'LUMO-PBE0', 'Gap-PBE0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_xyz(model, xyzfile):\n",
    "    return model.forward(\n",
    "        schnetpack.data.loader._collate_aseatoms([\n",
    "            schnetpack.data.atoms.torchify_dict(\n",
    "                schnetpack.data.atoms._convert_atoms(\n",
    "                    ase.io.read(xyzfile)\n",
    "                )\n",
    "            )\n",
    "        ])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dir(model_name):\n",
    "    return os.path.join(base_dir, 'models', model_name)\n",
    "def model_path(model_name):\n",
    "    return os.path.join(model_dir(model_name), 'best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, load the multiask_model:\n",
    "model_name = 'multitask_model_v01'\n",
    "model = torch.load(model_path(model_name), map_location=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on unified test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testset(dataset_name, select_tasks):\n",
    "    (_, _, test), mapping = multitask_data.split_dataset(\n",
    "        data_base_dir=data_base_dir,\n",
    "        dataset_name=dataset_name,\n",
    "        select_tasks=select_tasks)\n",
    "    return MultitaskAtomsData(test, mapping, validity_column=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_qm9 = testset('qm9', select_tasks=b3lyp_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_alchemy = testset('alchemy', select_tasks=b3lyp_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oe62 = testset('oe62', select_tasks=pbe0_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hopv = testset('hopv', select_tasks=b3lyp_tasks + pbe0_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"QM9_test\": test_qm9,\n",
    "    \"Alchemy_test\": test_alchemy,\n",
    "    \"OE62_test\": test_oe62,\n",
    "    \"HOPV_test\": test_hopv,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_properties(model):\n",
    "    try: # for Set2Set output module\n",
    "         return [p for om in model.output_modules for p in om.properties]\n",
    "    except: # for Atomwise output module\n",
    "        return [om.property for om in model.output_modules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_unified(model, dataset_name, n_points=None, seed=None):\n",
    "    dataset = datasets[dataset_name]\n",
    "    batch_size = 10\n",
    "    \n",
    "    gen = torch.Generator()\n",
    "    if seed:\n",
    "        gen.manual_seed(seed)\n",
    "    else:\n",
    "        gen.seed()\n",
    "    sampler = torch.utils.data.RandomSampler(dataset, replacement=False, generator=gen)\n",
    "    batch_sampler = torch.utils.data.BatchSampler(sampler, batch_size, drop_last=False)\n",
    "\n",
    "    ret = {\n",
    "        'tgt': {p: np.array([]) for p in dataset.available_properties},\n",
    "        'est': {p: np.array([]) for p in get_available_properties(model=model)},\n",
    "    }\n",
    "    test_loader = schnetpack.data.loader.AtomsLoader(dataset, batch_sampler=batch_sampler)\n",
    "    for i, b in enumerate(test_loader):\n",
    "        for p in ret['tgt'].keys():\n",
    "            ret['tgt'][p] = np.append(ret['tgt'][p], b[p])\n",
    "        b = {k: v.to(device) for k, v in b.items()}\n",
    "        est = model(b)\n",
    "        for p in ret['est'].keys():\n",
    "            ret['est'][p] = np.append(ret['est'][p], est[p].detach().to('cpu').numpy())\n",
    "        if n_points is not None and (i+1) * batch_size >= n_points:\n",
    "            break\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_regular_data(n_points=100, seed=None):\n",
    "    return {\n",
    "        dataset_name: evaluate_unified(\n",
    "            model, dataset_name, n_points, seed=seed) for dataset_name in datasets.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_est = compute_regular_data(n_points=10, seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on data of Kuzmich2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kuzmich(model, seed=None, n_points=-1):\n",
    "    kuzmich_dir = os.path.join(data_base_dir, 'Kuzmich2017')\n",
    "    df = pd.read_csv(os.path.join(kuzmich_dir, 'table1.csv'))\n",
    "    mapping = {\n",
    "        'DTDfBTTDPP2': 'DTDfBT(TDPP)2',\n",
    "        '10_DBFI-MTT': 'DBFI-MTT',\n",
    "    }\n",
    "    ambiguous = ['M10']\n",
    "    \n",
    "    # Get valid files and establish canonical order\n",
    "    files = {}\n",
    "    for f in sorted(os.listdir(os.path.join(kuzmich_dir, 'xyz'))):\n",
    "        if f.endswith('.xyz'):\n",
    "            id = f[3:-13]\n",
    "            if id in mapping:\n",
    "                id = mapping[id]\n",
    "            if id in ambiguous:\n",
    "                print('id: {} ambiguous'.format(f[3:-13]))\n",
    "                continue\n",
    "            lb = f[3:-13]\n",
    "            files[lb] = (f, id)\n",
    "    # Sort by keys:\n",
    "    fs = sorted(list(files.items()))\n",
    "\n",
    "    # Shuffle order\n",
    "    if seed is not None:\n",
    "        random_state = np.random.RandomState(seed=seed)\n",
    "        random_state.shuffle(fs)\n",
    "\n",
    "    # Compute only on the desired random subset\n",
    "    ret = {}\n",
    "    for lb, (f, id) in fs[:n_points]:\n",
    "        xyzfile = os.path.join(kuzmich_dir, 'xyz', f)\n",
    "        pred = predict_on_xyz(model, xyzfile)\n",
    "        est = {k: float(v) for k, v in pred.items()}\n",
    "        tgt = {\n",
    "            'LUMO-B3LYP': float(df[df['Acceptorâ€™s Label']==id]['LUMO (eV)'])\n",
    "        }\n",
    "        ret[lb] = {\n",
    "            'tgt': tgt,\n",
    "            'est': est,\n",
    "        }\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_est_kuzmich = evaluate_kuzmich(model, n_points=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_properties = get_available_properties(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Kuzmich2017 data alone:\n",
    "qe = 'LUMO-B3LYP' if 'LUMO-B3LYP' in est_properties else 'LUMO-PBE0'\n",
    "qt = 'LUMO-B3LYP'\n",
    "x = [v['tgt'][qt] for v in tgt_est_kuzmich.values()]\n",
    "y = [v['est'][qe] for v in tgt_est_kuzmich.values()]\n",
    "plt.scatter(x, y)\n",
    "plt.axline((np.mean(x), np.mean(x)), slope=1)\n",
    "plt.xlabel('{} target (eV)'.format(qt))\n",
    "plt.ylabel('{} estimate (eV)'.format(qe))\n",
    "plt.title(model_name)\n",
    "plt.show()\n",
    "dev = np.array(x) - np.array(y)\n",
    "print('MAE={:.2f}(eV), RMSE={:.2f}eV'.format(\n",
    "    np.mean(np.abs(dev)),\n",
    "    np.sqrt(np.mean(np.square(dev)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring data into the regular format:\n",
    "def add_kuzmich(tgt_est, seed=None, n_points=-1):\n",
    "    tgt_est_kuzmich = evaluate_kuzmich(model, seed=seed, n_points=n_points)\n",
    "    # Drop keys:\n",
    "    k_data = list(tgt_est_kuzmich.values())\n",
    "    ret = {\n",
    "            'tgt': {p: np.array([]) for p in k_data[0]['tgt'].keys()},\n",
    "            'est': {p: np.array([]) for p in k_data[0]['est'].keys()},\n",
    "        }\n",
    "    for kd in k_data:\n",
    "        for k in ret['tgt'].keys():\n",
    "            ret['tgt'][k] = np.append(ret['tgt'][k], [kd['tgt'][k]])\n",
    "        for k in ret['est'].keys():\n",
    "            ret['est'][k] = np.append(ret['est'][k], [kd['est'][k]])\n",
    "    tgt_est['Kuzmich2017'] = ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_kuzmich(tgt_est, seed=None, n_points=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlined evaluation and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOMSEED = 26463461"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v08'\n",
    "model = torch.load(model_path(model_name), map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v08_avg'\n",
    "model = torch.load(model_path(model_name), map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v08_sum'\n",
    "model = torch.load(model_path(model_name), map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v06'\n",
    "model = torch.load(model_path(model_name), map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_only_b3lyp'\n",
    "model = torch.load(model_path(model_name), map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v07'\n",
    "model = torch.load(model_path(model_name), map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v01'\n",
    "model = torch.load(model_path(model_name), map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v01_sum'\n",
    "model = torch.load(model_path(model_name), map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v01b_avg'\n",
    "model = torch.load(model_path(model_name), map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v05'\n",
    "model = torch.load(model_path(model_name), map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_only_pbe0'\n",
    "model = torch.load(model_path(model_name), map_location=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute and dump the full error distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_file = os.path.join(model_dir(model_name), 'deviations.npz')\n",
    "summary_file = os.path.join(model_dir(model_name), 'deviations_summary.json')\n",
    "if not os.path.exists(summary_file) and not os.path.exists(target_file):\n",
    "    est_properties = get_available_properties(model=model)\n",
    "    tgt_est = compute_regular_data(n_points=None, seed=RANDOMSEED)\n",
    "    add_kuzmich(tgt_est, seed=RANDOMSEED)\n",
    "    devs = {}\n",
    "    for test, data in tgt_est.items():\n",
    "        print(test)\n",
    "        for p in data['tgt'].keys():\n",
    "            if p in data['est']:\n",
    "                print('  ', p)\n",
    "                devs[test + ':' + p] = data['est'][p] - data['tgt'][p]\n",
    "    np.savez(target_file, **devs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the deviations (DataFrame, json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(summary_file):\n",
    "    devs = np.load(target_file)\n",
    "    import pandas as pd\n",
    "    summary = pd.DataFrame(columns=['test', 'property', 'mean(error)', 'std(error)', 'MAE', 'RMSE', 'size'])\n",
    "    for k, dev in devs.items():\n",
    "        test, p = k.split(':')\n",
    "        summary = summary.append({\n",
    "            'test': test,\n",
    "            'property': p,\n",
    "            'mean(error)': np.mean(dev),\n",
    "            'std(error)': np.std(dev),\n",
    "            'MAE': np.mean(np.abs(dev)),\n",
    "            'RMSE': np.sqrt(np.mean(np.square(dev))),\n",
    "            'size': len(dev),\n",
    "        }, ignore_index=True)\n",
    "    summary.to_json(summary_file, indent=2, orient='records')\n",
    "else:\n",
    "    summary = pd.read_json(os.path.join(model_dir(model_name), 'deviations_summary.json'))\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 25\n",
    "est_properties = get_available_properties(model=model)\n",
    "tgt_est = compute_regular_data(n_points=n_points, seed=RANDOMSEED)\n",
    "add_kuzmich(tgt_est, seed=RANDOMSEED, n_points=n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a fixed color code for each test set\n",
    "for k, color in {\n",
    "    'QM9_test': 'orange',\n",
    "    'Alchemy_test': 'red',\n",
    "    'OE62_test': 'purple',\n",
    "    'HOPV_test': 'blue',\n",
    "    'TABS': 'green',\n",
    "    'Kuzmich2017': 'black'\n",
    "}.items():\n",
    "    if k in tgt_est:\n",
    "        tgt_est[k]['color'] = color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = ['HOMO', 'LUMO', 'Gap']\n",
    "theories = ['B3LYP', 'PBE0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(tgt_est: dict, qt_tgt: str, qt_est: str, n_points: int = -1, skiptests=[]):\n",
    "    def measure_errors(x, y):\n",
    "        dev = np.array(x) - np.array(y)\n",
    "        mae = '{:.2f}eV'.format(np.mean(np.abs(dev)))\n",
    "        rmse = '{:.2f}eV'.format(np.sqrt(np.mean(np.square(dev))))\n",
    "        return mae, rmse\n",
    "    def lookup_errors(test, prop):\n",
    "        test_row = summary[(summary['test']==test) & (summary['property']==prop)]\n",
    "        assert len(test_row) == 1\n",
    "        mae = '{:.3f}eV'.format(float(test_row['MAE']))\n",
    "        rmse = '{:.3f}eV'.format(float(test_row['RMSE']))\n",
    "        return mae, rmse\n",
    "\n",
    "    plot_empty = True\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plotname = '{}-{}'.format(model_name, qt_tgt);\n",
    "    if qt_est != qt_tgt:\n",
    "        plotname += '-cross'\n",
    "    for dataset_name, te in tgt_est.items():\n",
    "        try:\n",
    "            x = te['tgt'][qt_tgt]\n",
    "            y = te['est'][qt_est]\n",
    "        except:\n",
    "            continue\n",
    "        if dataset_name in skiptests:\n",
    "            # Add a tag, but only if skipped due to skiptests: \n",
    "            plotname = '{}-skip{}'.format(plotname, dataset_name)\n",
    "            continue\n",
    "        if qt_tgt == qt_est:\n",
    "            try:\n",
    "                mae, rmse = lookup_errors(test=dataset_name, prop=qt_est)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"\"\"Something went wrong looking up {}, {}. Measure \n",
    "                errors from plot data.\"\"\".format(dataset_name, qt_est))\n",
    "                mae, rmse = measure_errors(x, y)\n",
    "        else:\n",
    "            mae, rmse = measure_errors(x, y)\n",
    "        print('{}: MAE={}, RMSE={}'.format(dataset_name, mae, rmse))\n",
    "        plt.scatter(x[:n_points], y[:n_points], color=te['color'], label='{dataset} (MAE={mae})'.format(\n",
    "            dataset=dataset_name, mae=mae))\n",
    "        plt.axline((np.mean(x), np.mean(x)), slope=1)\n",
    "        plt.xlabel('{} target (eV)'.format(qt_tgt))\n",
    "        plt.ylabel('{} estimate (eV)'.format(qt_est))\n",
    "        plot_empty = False\n",
    "    if plot_empty:\n",
    "        print(\"{}/{} empty for {}.\".format(qt_tgt, qt_est, model_name))\n",
    "    else:\n",
    "        plt.title(model_name)\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        tgt_dir = os.path.join(base_dir, 'figures', 'tgt-est')\n",
    "        os.makedirs(tgt_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(tgt_dir, '{}.pdf'.format(plotname)), dpi=200)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target-estimate plots for each property and theory (diagonal and cross)\n",
    "for a in properties:\n",
    "    for t in theories:\n",
    "        assert len(theories)==2\n",
    "        t_cross = [th for th in theories if th != t][0]\n",
    "        q = a + '-' + t\n",
    "        q_cross = a + '-' + t_cross\n",
    "        plt.rcParams.update({'axes.facecolor': 'lightgray'})\n",
    "        make_plot(tgt_est, q, q_cross, n_points=n_points)\n",
    "        plt.rcParams.update({'axes.facecolor': 'white'})\n",
    "        make_plot(tgt_est, q, q, n_points=n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schnetpack_conda",
   "language": "python",
   "name": "schnetpack_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
