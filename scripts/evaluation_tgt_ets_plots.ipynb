{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the estimation errors of the trained models\n",
    "Assume:\n",
    "- that the models are already trained.\n",
    "- that the compute_errors script has been execuded.\n",
    "\n",
    "Thus, in the model_base_dir, there are subfolders, one for each model configuration, containing:\n",
    "- best_model_state.pth: the state dictionary of the trained model\n",
    "- deviations.npz: Target-estimate values for all data points\n",
    "- deviations_summary.json: Previously computed error statistics (evaluated on the full test sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "base_dir = '..'\n",
    "data_base_dir = os.path.join(base_dir, 'data')\n",
    "model_base_dir = os.path.join(base_dir, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dir(model_name):\n",
    "    return os.path.join(model_base_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    mdir = model_dir(model_name)\n",
    "    state_path = os.path.join(mdir, 'best_model_state.pth')\n",
    "    if os.path.exists(state_path):\n",
    "        module_name = 'configs.{}'.format(model_name)\n",
    "        print('Importing module {}...'.format(module_name))\n",
    "        config = importlib.import_module(module_name)\n",
    "        model = config.build_model()\n",
    "        print('Loading', state_path)\n",
    "        model.load_state_dict(torch.load(state_path))\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "    else:\n",
    "        raise RuntimeError('model_name {} not found at {}'.format(\n",
    "                model_name, mdir))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v08_sum'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v08_avg'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v08'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v06'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v07'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v01'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v05'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v01_avg'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v01_sum'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v01_only_pbe0'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multitask_model_v01_only_b3lyp'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the deviations (DataFrame, json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_file = os.path.join(model_dir(model_name), 'deviations_summary.json')\n",
    "if os.path.exists(summary_file):\n",
    "    summary = pd.read_json(summary_file)\n",
    "else:\n",
    "    print('Please run the \"compute_errors\" script for model {}'.format(model_name))\n",
    "    summary = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 25\n",
    "RANDOMSEED = 26463461 # For shuffling the items in a reproducible way\n",
    "est_properties = evaluation.get_available_properties(model=model)\n",
    "tgt_est = evaluation.compute_regular_data(\n",
    "    model,\n",
    "    data_base_dir=data_base_dir,\n",
    "    n_points=n_points, seed=RANDOMSEED,\n",
    "    device=device)\n",
    "evaluation.add_kuzmich(\n",
    "    tgt_est,\n",
    "    model, data_base_dir,\n",
    "    seed=RANDOMSEED, n_points=n_points, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a fixed color code for each test set\n",
    "for k, color in {\n",
    "    'qm9': 'orange',\n",
    "    'alchemy': 'red',\n",
    "    'oe62': 'purple',\n",
    "    'hopv': 'blue',\n",
    "    'Kuzmich2017': 'black'\n",
    "}.items():\n",
    "    if k in tgt_est:\n",
    "        tgt_est[k]['color'] = color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = ['HOMO', 'LUMO', 'Gap']\n",
    "theories = ['B3LYP', 'PBE0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(tgt_est: dict, qt_tgt: str, qt_est: str, n_points: int = -1):\n",
    "    def measure_errors(x, y):\n",
    "        dev = np.array(x) - np.array(y)\n",
    "        mae = '{:.2f}eV'.format(np.mean(np.abs(dev)))\n",
    "        rmse = '{:.2f}eV'.format(np.sqrt(np.mean(np.square(dev))))\n",
    "        return mae, rmse\n",
    "    def lookup_errors(test, prop):\n",
    "        test_row = summary[(summary['test']==test) & (summary['property']==prop)]\n",
    "        assert len(test_row) == 1\n",
    "        mae = '{:.3f}eV'.format(float(test_row['MAE']))\n",
    "        rmse = '{:.3f}eV'.format(float(test_row['RMSE']))\n",
    "        return mae, rmse\n",
    "\n",
    "    plot_empty = True\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plotname = '{}-{}'.format(model_name, qt_tgt);\n",
    "    if qt_est != qt_tgt:\n",
    "        plotname += '-cross'\n",
    "    for dataset_name, te in tgt_est.items():\n",
    "        try:\n",
    "            x = te['tgt'][qt_tgt]\n",
    "            y = te['est'][qt_est]\n",
    "        except:\n",
    "            continue\n",
    "        if qt_tgt == qt_est:\n",
    "            try:\n",
    "                mae, rmse = lookup_errors(test=dataset_name, prop=qt_est)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"\"\"Something went wrong looking up {}, {}. Measure \n",
    "                errors from plot data.\"\"\".format(dataset_name, qt_est))\n",
    "                mae, rmse = measure_errors(x, y)\n",
    "        else:\n",
    "            mae, rmse = measure_errors(x, y)\n",
    "        print('{}: MAE={}, RMSE={}'.format(dataset_name, mae, rmse))\n",
    "        plt.scatter(x[:n_points], y[:n_points], color=te['color'], label='{dataset} (MAE={mae})'.format(\n",
    "            dataset=dataset_name, mae=mae))\n",
    "        plt.axline((np.mean(x), np.mean(x)), slope=1)\n",
    "        plt.xlabel('{} target (eV)'.format(qt_tgt))\n",
    "        plt.ylabel('{} estimate (eV)'.format(qt_est))\n",
    "        plot_empty = False\n",
    "    if plot_empty:\n",
    "        print(\"{}/{} empty for {}.\".format(qt_tgt, qt_est, model_name))\n",
    "    else:\n",
    "        plt.title(model_name)\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        tgt_dir = os.path.join(base_dir, 'figures', 'tgt-est')\n",
    "        os.makedirs(tgt_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(tgt_dir, '{}.pdf'.format(plotname)), dpi=200)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target-estimate plots for each property and theory (diagonal and cross)\n",
    "for a in properties:\n",
    "    for t in theories:\n",
    "        assert len(theories)==2\n",
    "        t_cross = [th for th in theories if th != t][0]\n",
    "        q = a + '-' + t\n",
    "        q_cross = a + '-' + t_cross\n",
    "        plt.rcParams.update({'axes.facecolor': 'lightgray'})\n",
    "        make_plot(tgt_est, q, q_cross, n_points=n_points)\n",
    "        plt.rcParams.update({'axes.facecolor': 'white'})\n",
    "        make_plot(tgt_est, q, q, n_points=n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schnetpack-pip",
   "language": "python",
   "name": "schnetpack-pip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
